# Store estimated curves
shapes.smsp[, k] <- fit.smsp$mu
shapes.pensp[, k] <- fit.pensp$mu
# shapes.lspensp[, k] <- fit.lspensp$mu
# shapes.huber[, k] <- fit.huber$mu
# shapes.sfpca[, k] <- try(mu.sfpca)
# shapes.sfpca[, k] <- tryCatch(
#   mu.sfpca,
#   error = function(e) rep(NA, nrow(shapes.sfpca))
# )
}
# mean(mse.sfpca[mse.sfpca!=0])*1000; 1000 * sd(mse.sfpca[mse.sfpca!=0], na.rm = TRUE)/sqrt(nsim)
# Summarize Results
# Mean and standard error of MSE
mean(mse.smsp, na.rm = TRUE) * 1000; 1000 * sd(mse.smsp, na.rm = TRUE)/sqrt(nsim)
mean(mse.pensp, na.rm = TRUE) * 1000; 1000 * sd(mse.pensp, na.rm = TRUE)/sqrt(nsim)
mean(mse.lspensp, na.rm = TRUE) * 1000; 1000 * sd(mse.lspensp, na.rm = TRUE)/sqrt(nsim)
mean(mse.huber, na.rm = TRUE) * 1000; 1000 * sd(mse.huber, na.rm = TRUE)/sqrt(nsim)
# Visualize Estimated Curves Across Simulations
par(mar = c(4,3.5,2,2), mgp = c(1.5, 0.75, 0), mfrow = c(1,1))
matplot(t_grid, shapes.pensp, lwd = 3, col = "gray", type = "l", lty = 1, cex.lab = 1.5, cex.axis = 1.5,
ylab = "", xlab = "t", ylim = c(-1,1)) # ylim = c(-1,1) or ylim = c(-0.5,1.3)
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
par(mar = c(4,3.5,2,2), mgp = c(1.5, 0.75, 0), mfrow = c(1,1))
matplot(t_grid, shapes.lspensp, lwd = 3, col = "gray", type = "l", lty = 1,cex.lab = 1.5, cex.axis = 1.5,
ylab = "", xlab = "t", ylim = c(-1, 1))
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
par(mfrow = c(1, 1))
matplot(t_grid, shapes.huber, lwd = 3, col = "gray", type = "l", lty = 1)
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
# ----------------------------------------------------------------------
# Simulation Study: Section 4 of Kalogridis (2025+)
# ----------------------------------------------------------------------
# This script simulates discretely sampled functional data, adds
# noise, and compares three estimators:
#   1. Smoothing-spline quantile estimator (quan_smsp)
#   2. Least-squares penalized spline estimator (ls_pensp)
#   3. Quantile penalized spline estimator (quan_pensp)
#   4. Huber penalized spline estimator (huber_pensp)
# ----------------------------------------------------------------------
setwd("C:/Users/ik77w/OneDrive - University of Glasgow/Documents/GitHub/Robust-Functional-Location-Estimation")
library(sparseFPCA)
# Number of simulations (500 in the paper)
nsim <- 500
# Number of subjects and number of measurement points
n <- 100 # 50
p <- 50 # 30
# Initialize storage for mean squared errors
mse.smsp <- mse.pensp <- mse.lspensp <- mse.huber <- rep(0, nsim)
mse.sfpca <- rep(0, nsim)
# Initialize storage for estimated curves
shapes.smsp <- shapes.pensp <- shapes.lspensp <- shapes.huber <- matrix(0, ncol = nsim, nrow = p)
shapes.sfpca <- matrix(0, ncol = nsim, nrow = p)
# Grid of measurement points
t_grid <- seq(0, 1, length.out = p)
# Define true population mean function
# mu_true <- function(t) sin(2 * pi * t) # Easy example
# Harder example (mu_2 in the paper)
mu_true <- function(t) exp(-(t-0.25)^2/0.01)+exp(-(t-0.50)^2/0.01)+exp(-(t-0.75)^2/0.01)
mu_grid <- mu_true(t_grid)
# Main Simulation Loop
for(k in 1:nsim){
print(paste("Simulation", k, "of", nsim))
# Generate functional data for n subjects
X <- matrix(NA, nrow = n, ncol = p) # discretely sampled data
Y <- matrix(NA, nrow = n, ncol = p) # discretely sampled data with measurement error
# DataSFPCA <- vector("list", 2)
# names(DataSFPCA) <- c("x", "pp")
for(i in 1:n){
X[i,] <- mu_grid
for(j in 1:50){
X[i,] <- X[i,] + sqrt(2) * rt(1, df = 1) *
sapply(t_grid, FUN = function(x) sin((j-0.5)*pi*x)/((j-0.5)*pi))
}
# Randomly sample a subset of measurement points for subject i
m_i <- sample(floor(0.5 * p):floor(0.8 * p), 1)
idx <- sort(sample(seq_len(p), m_i))
# Add heavy-tailed noise
# zeta <- 0.5 * rt(m_i, df = 5)
zeta <- 0
Y[i, idx] <- X[i, idx] + zeta
# DataSFPCA$x[[i]] <- Y[i, idx]
# DataSFPCA$pp[[i]] <- idx/p
}
# fit.sfpca <- tryCatch(
#   efpca(
#     X       = DataSFPCA,
#     hs.mu   = exp(seq(log(1e-04), log(1), length.out = 20)),
#     opt.h.cov = 4
#   ),
#   error = function(e) list(muh = NA)
# )
# fit.sfpca <- tryCatch(efpca(X=DataSFPCA, hs.mu = exp(seq(log(1e-04), log(1), length.out = 20)), opt.h.cov = 4),
# error = function(e) list(a = NA, b = NA))
# u1 <- try(unlist(fit.sfpca$muh))      # all elements of list2
# u2 <- try(unlist(DataSFPCA$pp))
# u1 <- tryCatch(
#   unlist(fit.sfpca$muh),
#   error = function(e) rep(NA, length(DataSFPCA$pp))   # correct length placeholder
# )
#
# u2 <- tryCatch(
#   unlist(DataSFPCA$pp),
#   error = function(e) rep(NA, length(u1))
# )
# sorted_unique <- tryCatch(
#   sort(unique(u2)),
#   error = function(e) NA
# )
## ----- 4. Compute mu.sfpca safely -----
# mu.sfpca <- tryCatch(
#   u1[match(sorted_unique, u2)],
#   error = function(e) rep(NA, length(sorted_unique))
# )
# sorted_unique <- try(sort(unique(u2)))
# mu.sfpca <- try(u1[match(sorted_unique, u2)])
# plot(t_grid, mu.sfpca)
# # par(mar = c(4,3.5,2,2), mgp = c(1.5, 0.75, 0), mfrow = c(1,1))
# matplot(t_grid,t(X), pch = 20, col = "gray", cex = 1.1, xlab = "t",
# ylab = "", ylim = c(-4.5, 4.5), cex.lab = 1.5, cex.axis = 1.5) ; grid()
# lines(t_grid, mu_grid, lwd = 3, col = "black")
# Fit estimators
fit.smsp <- quan_smsp(Y, alpha = 0.5)
# fit.lspensp <- ls_pensp(Y)            # Least-squares penalized spline
fit.pensp  <- quan_pensp(Y, alpha = 0.5)  # Quantile penalized spline
# fit.huber <- huber_pensp(Y)
# -------------------------------
# Plot a single simulation
# -------------------------------
# par(mar = c(4,3.5,2,2), mgp = c(3, 1.5, 0), mfrow = c(1,1))
# plot(t_grid, mu_grid, lwd = 3, lty = 1, type = "l", cex.axis = 2.5, cex.lab = 2.5,
# ylab = "", xlab = "t", ylim = c(-1.2, 1.2)); grid()
# lines(t_grid, fit.huber$mu, lwd = 3, col = "blue")
# lines(t_grid, fit.pensp$mu, lwd = 3, col = "blue")
# lines(t_grid, fit.lspensp$mu, lwd = 3, col = "red")
# -------------------------------
# Compute Mean Squared Errors
# -------------------------------
mse.smsp[k] <- mean((fit.smsp$mu - mu_grid)^2)
mse.pensp[k] <- mean((fit.pensp$mu - mu_grid)^2)
# mse.lspensp[k] <- mean((fit.lspensp$mu - mu_grid)^2)
# mse.huber[k] <- mean((fit.huber$mu - mu_grid)^2)
# mse.sfpca[k] <- try(mean((mu.sfpca- mu_grid)^2))
# mse.sfpca[k] <- tryCatch(
#   mean((mu.sfpca - mu_grid)^2),
#   error = function(e) NA
# )
# Store estimated curves
shapes.smsp[, k] <- fit.smsp$mu
shapes.pensp[, k] <- fit.pensp$mu
# shapes.lspensp[, k] <- fit.lspensp$mu
# shapes.huber[, k] <- fit.huber$mu
# shapes.sfpca[, k] <- try(mu.sfpca)
# shapes.sfpca[, k] <- tryCatch(
#   mu.sfpca,
#   error = function(e) rep(NA, nrow(shapes.sfpca))
# )
}
# mean(mse.sfpca[mse.sfpca!=0])*1000; 1000 * sd(mse.sfpca[mse.sfpca!=0], na.rm = TRUE)/sqrt(nsim)
# Summarize Results
# Mean and standard error of MSE
mean(mse.smsp, na.rm = TRUE) * 1000; 1000 * sd(mse.smsp, na.rm = TRUE)/sqrt(nsim)
mean(mse.pensp, na.rm = TRUE) * 1000; 1000 * sd(mse.pensp, na.rm = TRUE)/sqrt(nsim)
mean(mse.lspensp, na.rm = TRUE) * 1000; 1000 * sd(mse.lspensp, na.rm = TRUE)/sqrt(nsim)
mean(mse.huber, na.rm = TRUE) * 1000; 1000 * sd(mse.huber, na.rm = TRUE)/sqrt(nsim)
# Visualize Estimated Curves Across Simulations
par(mar = c(4,3.5,2,2), mgp = c(1.5, 0.75, 0), mfrow = c(1,1))
matplot(t_grid, shapes.pensp, lwd = 3, col = "gray", type = "l", lty = 1, cex.lab = 1.5, cex.axis = 1.5,
ylab = "", xlab = "t", ylim = c(-1,1)) # ylim = c(-1,1) or ylim = c(-0.5,1.3)
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
par(mar = c(4,3.5,2,2), mgp = c(1.5, 0.75, 0), mfrow = c(1,1))
matplot(t_grid, shapes.lspensp, lwd = 3, col = "gray", type = "l", lty = 1,cex.lab = 1.5, cex.axis = 1.5,
ylab = "", xlab = "t", ylim = c(-1, 1))
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
par(mfrow = c(1, 1))
matplot(t_grid, shapes.huber, lwd = 3, col = "gray", type = "l", lty = 1)
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
# ----------------------------------------------------------------------
# Simulation Study: Section 4 of Kalogridis (2025+)
# ----------------------------------------------------------------------
# This script simulates discretely sampled functional data, adds
# noise, and compares 4 estimators:
#   1. LAD smoothing spline estimator (quan_smsp)
#   2. Least-squares penalized spline estimator (ls_pensp)
#   3. LAD penalized spline estimator (quan_pensp)
#   4. Huber penalized spline estimator (huber_pensp)
# ----------------------------------------------------------------------
# Number of simulations (500 in the paper)
nsim <- 5
# Number of subjects and number of measurement points
n <- 100 # 50
p <- 50 # 30
# Initialize storage for mean squared errors
mse.smsp <- mse.pensp <- mse.lspensp <- mse.huber <- rep(0, nsim)
# Initialize storage for estimated curves
shapes.smsp <- shapes.pensp <- shapes.lspensp <- shapes.huber <- matrix(0, ncol = nsim, nrow = p)
shapes.sfpca <- matrix(0, ncol = nsim, nrow = p)
# Grid of measurement points
t_grid <- seq(0, 1, length.out = p)
# True population mean functions
mu_true <- function(t) sin(2 * pi * t) # Easy example
# Harder example (mu_2 in the paper)
# mu_true <- function(t) exp(-(t-0.25)^2/0.01)+exp(-(t-0.50)^2/0.01)+exp(-(t-0.75)^2/0.01)
mu_grid <- mu_true(t_grid)# Evaluate at grid
# Main Simulation Loop
for(k in 1:nsim){
print(paste("Simulation", k, "of", nsim))
# Generate functional data for n subjects
X <- matrix(NA, nrow = n, ncol = p) # discretely sampled data
Y <- matrix(NA, nrow = n, ncol = p) # discretely sampled data with measurement error
for(i in 1:n){
X[i,] <- mu_grid
for(j in 1:50){
X[i,] <- X[i,] + sqrt(2) * rt(1, df = 5) * # or df = 1 for very heavy-tailed curves
sapply(t_grid, FUN = function(x) sin((j-0.5)*pi*x)/((j-0.5)*pi))
}
# Randomly sample a subset of measurement points for subject i
m_i <- sample(floor(0.5 * p):floor(0.8 * p), 1)
idx <- sort(sample(seq_len(p), m_i))
# Add heavy-tailed noise or set zeta = 0
zeta <- 0.5 * rt(m_i, df = 5)
# zeta <- 0
Y[i, idx] <- X[i, idx] + zeta
}
# Fit estimators
fit.smsp <- quan_smsp(Y) # LAD smoothing splines
fit.lspensp <- ls_pensp(Y)            # Least squares penalized splines
fit.pensp  <- quan_pensp(Y)  # LAD penalized splines
fit.huber <- huber_pensp(Y) # Huber penalized splines
# Plot a single simulation
par(mar = c(4,3.5,2,2), mgp = c(3, 1.5, 0), mfrow = c(1,1))
plot(t_grid, mu_grid, lwd = 3, lty = 1, type = "l", cex.axis = 2.5, cex.lab = 2.5,
ylab = "", xlab = "t", ylim = c(-1.2, 1.2)); grid()
lines(t_grid, fit.huber$mu, lwd = 3, col = "blue")
lines(t_grid, fit.pensp$mu, lwd = 3, col = "orange")
lines(t_grid, fit.lspensp$mu, lwd = 3, col = "red")
# Compute Mean Squared Errors
mse.smsp[k] <- mean((fit.smsp$mu - mu_grid)^2)
mse.pensp[k] <- mean((fit.pensp$mu - mu_grid)^2)
mse.lspensp[k] <- mean((fit.lspensp$mu - mu_grid)^2)
mse.huber[k] <- mean((fit.huber$mu - mu_grid)^2)
# Store estimated curves
shapes.smsp[, k] <- fit.smsp$mu
shapes.pensp[, k] <- fit.pensp$mu
shapes.lspensp[, k] <- fit.lspensp$mu
shapes.huber[, k] <- fit.huber$mu
}
# ----------------------------------------------------------------------
# ls_pensp: Least-Squares Penalized Spline Estimator
# ----------------------------------------------------------------------
# This function estimates the mean function of discretely sampled
# functional data using B-splines and an O-spline roughness penalty.
# Computation is performed via a fast C++ routine.
# Please find detailed documentation below.
# ----------------------------------------------------------------------
require(fda)           # For B-spline basis and penalty functions
require(Rcpp)          # Interface to C++
require(RcppArmadillo) # Efficient matrix operations in C++
Rcpp::sourceCpp("combined.cpp")  # Load the C++ functions
ls_pensp <- function(Y, r = 2, m = 4, K = NULL,
lambda_grid = exp(seq(log(1e-8), log(1e-1), length.out = 50))) {
# --- Preprocessing ---
# Convert input to matrix if needed and remove empty rows
Y <- as.matrix(Y)
Y <- Y[rowSums(!is.na(Y)) > 0, , drop = FALSE]
n <- nrow(Y)  # number of subjects
p <- ncol(Y)  # number of measurement points per subject
# Create a uniform grid for the measurements (0 to 1)
t_grid <- seq(0, 1, length.out = p)
# Map each observation to its discretization point
T_mat <- matrix(rep(t_grid, each = n), nrow = n)
obs_idx <- which(!is.na(Y))      # indices of observed entries
t_obs <- T_mat[obs_idx]          # time points of observed entries
y_obs <- as.numeric(Y[obs_idx])  # observed values
# --- Weights
# Compute number of measurements per subject
m_i <- rowSums(!is.na(Y))
K <- ifelse(is.null(K), min(35, max(m_i)), K)
# Map linear indices back to row IDs
row_id <- ((obs_idx - 1) %% n) + 1
# Assign weight 1/(n * m_i) to each observation
weights_per_obs <- 1 / (n * m_i[row_id])
# --- Basis Construction
# Use B-spline basis with K basis functions from equidistant knots and order m
b_basis <- create.bspline.basis(rangeval = c(0, 1), nbasis = K, norder = m)
# Evaluate B-spline basis at observed times
B <- eval.basis(t_obs, b_basis)
# --- Penalty Matrix ---
# Roughness penalty on r-th derivative
Pen <- bsplinepen(b_basis, Lfdobj = r)
# Solve penalized least squares using the fast C++ routine
res <- ls_pensp_cpp2(B, Pen, y_obs, weights_per_obs, lambda_grid)
# Compute estimated mean function on full grid
mu_est <- as.numeric(eval.basis(t_grid, b_basis) %*% res$beta)
# --- Return Results ---
return(list(
mu = mu_est,                  # estimated mean function
beta = res$beta,              # estimated B-spline coefficients
lambda = res$lambda,          # selected smoothing parameter
gcv = res$gcv,                # GCV values over lambda grid
t_grid = t_grid,              # grid of evaluation points
basis = b_basis,              # B-spline basis object
Penalty = Pen,                # penalty matrix
weights_per_obs = weights_per_obs  # observation weights
))
}
fit.lspensp <- ls_pensp(Y)            # Least squares penalized splines
fit.pensp  <- quan_pensp(Y)  # LAD penalized splines
fit.huber <- huber_pensp(Y) # Huber penalized splines
# ----------------------------------------------------------------------
# huber_pensp: Huber Penalized Spline Estimator
# ----------------------------------------------------------------------
# This function estimates the conditional Huber functional of
# discretely sampled functional data using B-splines and an O-spline roughness
# penalty. Computation is performed via a fast C++ routine.
# For details, please see the documentation below.
# ----------------------------------------------------------------------
require(fda)           # For B-spline basis and penalty functions
require(Rcpp)          # Interface to C++
require(RcppArmadillo) # Efficient matrix operations in C++
Rcpp::sourceCpp("combined.cpp")  # Load the C++ functions
huber_pensp <- function(Y, r = 2, m = 4, K = NULL,
lambda_grid = exp(seq(log(1e-8), log(1e-1), length.out = 50)),
max_it = 100, tol = 1e-6, tun = 1.345) {
# - Preprocessing -
# Convert input to matrix and remove rows with all NA
Y <- as.matrix(Y)
Y <- Y[rowSums(!is.na(Y)) > 0, , drop = FALSE]
n <- nrow(Y)  # number of subjects
p <- ncol(Y)  # number of measurement points per subject
# Uniform grid of measurement points from 0 to 1
t_grid <- seq(0, 1, length.out = p)
# Map observations to their time points
T_mat <- matrix(rep(t_grid, each = n), nrow = n)
obs_idx <- which(!is.na(Y))      # indices of observed entries
t_obs <- T_mat[obs_idx]          # time points of observed entries
y_obs <- Y[obs_idx]              # observed values
# --- Weights ---
# Number of measurements per subject
m_i <- rowSums(!is.na(Y))
K <- ifelse(is.null(K), min(35, max(m_i)), K)
# Map linear indices back to row IDs
row_id <- ((obs_idx - 1) %% n) + 1
# Assign weight 1/(n * m_i) to each observation
weights_per_obs <- 1 / (n * m_i[row_id])
# --- Basis Construction ---
# Create B-spline basis with K basis functions and order m
b_basis <- create.bspline.basis(rangeval = c(0, 1), nbasis = K, norder = m)
# Evaluate B-spline basis at observed points
B <- eval.basis(t_obs, b_basis)
# Penalty Matrix from the fda package
# Roughness penalty on r-th derivative
Pen <- bsplinepen(b_basis, Lfdobj = r)
# IRLS + GCV
# Fit the penalized Huber regression using the C++ routine
fit <- irls_gcv_cpp_huber(B, Pen, y_obs, weights_per_obs,
lambda_grid, max_it, tol = tol, tuning = tun)
# Estimated quantile function on full grid
mu_est <- eval.basis(t_grid, b_basis) %*% fit$beta_hat
return(list(
mu = mu_est,       # estimated Huber functional
lambda = fit$lambda,  # selected smoothing parameter
weights = fit$weights # observation weights used in IRLS
))
}
fit.huber <- huber_pensp(Y) # Huber penalized splines
# Plot a single simulation
par(mar = c(4,3.5,2,2), mgp = c(3, 1.5, 0), mfrow = c(1,1))
plot(t_grid, mu_grid, lwd = 3, lty = 1, type = "l", cex.axis = 2.5, cex.lab = 2.5,
ylab = "", xlab = "t", ylim = c(-1.2, 1.2)); grid()
lines(t_grid, fit.huber$mu, lwd = 3, col = "blue")
lines(t_grid, fit.pensp$mu, lwd = 3, col = "orange")
lines(t_grid, fit.lspensp$mu, lwd = 3, col = "red")
# ----------------------------------------------------------------------
# Simulation Study: Section 4 of Kalogridis (2025+)
# ----------------------------------------------------------------------
# This script simulates discretely sampled functional data, adds
# noise, and compares 4 estimators:
#   1. LAD smoothing spline estimator (quan_smsp)
#   2. Least-squares penalized spline estimator (ls_pensp)
#   3. LAD penalized spline estimator (quan_pensp)
#   4. Huber penalized spline estimator (huber_pensp)
# ----------------------------------------------------------------------
# Number of simulations (500 in the paper)
nsim <- 5
# Number of subjects and number of measurement points
n <- 100 # 50
p <- 50 # 30
# Initialize storage for mean squared errors
mse.smsp <- mse.pensp <- mse.lspensp <- mse.huber <- rep(0, nsim)
# Initialize storage for estimated curves
shapes.smsp <- shapes.pensp <- shapes.lspensp <- shapes.huber <- matrix(0, ncol = nsim, nrow = p)
shapes.sfpca <- matrix(0, ncol = nsim, nrow = p)
# Grid of measurement points
t_grid <- seq(0, 1, length.out = p)
# True population mean functions
mu_true <- function(t) sin(2 * pi * t) # Easy example
# Harder example (mu_2 in the paper)
# mu_true <- function(t) exp(-(t-0.25)^2/0.01)+exp(-(t-0.50)^2/0.01)+exp(-(t-0.75)^2/0.01)
mu_grid <- mu_true(t_grid)# Evaluate at grid
# Main Simulation Loop
for(k in 1:nsim){
print(paste("Simulation", k, "of", nsim))
# Generate functional data for n subjects
X <- matrix(NA, nrow = n, ncol = p) # discretely sampled data
Y <- matrix(NA, nrow = n, ncol = p) # discretely sampled data with measurement error
for(i in 1:n){
X[i,] <- mu_grid
for(j in 1:50){
X[i,] <- X[i,] + sqrt(2) * rt(1, df = 5) * # or df = 1 for very heavy-tailed curves
sapply(t_grid, FUN = function(x) sin((j-0.5)*pi*x)/((j-0.5)*pi))
}
# Randomly sample a subset of measurement points for subject i
m_i <- sample(floor(0.5 * p):floor(0.8 * p), 1)
idx <- sort(sample(seq_len(p), m_i))
# Add heavy-tailed noise or set zeta = 0
zeta <- 0.5 * rt(m_i, df = 5)
# zeta <- 0
Y[i, idx] <- X[i, idx] + zeta
}
# Fit estimators
fit.smsp <- quan_smsp(Y) # LAD smoothing splines
fit.lspensp <- ls_pensp(Y)            # Least squares penalized splines
fit.pensp  <- quan_pensp(Y)  # LAD penalized splines
fit.huber <- huber_pensp(Y) # Huber penalized splines
# Plot a single simulation
par(mar = c(4,3.5,2,2), mgp = c(3, 1.5, 0), mfrow = c(1,1))
plot(t_grid, mu_grid, lwd = 3, lty = 1, type = "l", cex.axis = 2.5, cex.lab = 2.5,
ylab = "", xlab = "t", ylim = c(-1.2, 1.2)); grid()
lines(t_grid, fit.huber$mu, lwd = 3, col = "blue")
lines(t_grid, fit.pensp$mu, lwd = 3, col = "orange")
lines(t_grid, fit.lspensp$mu, lwd = 3, col = "red")
# Compute Mean Squared Errors
mse.smsp[k] <- mean((fit.smsp$mu - mu_grid)^2)
mse.pensp[k] <- mean((fit.pensp$mu - mu_grid)^2)
mse.lspensp[k] <- mean((fit.lspensp$mu - mu_grid)^2)
mse.huber[k] <- mean((fit.huber$mu - mu_grid)^2)
# Store estimated curves
shapes.smsp[, k] <- fit.smsp$mu
shapes.pensp[, k] <- fit.pensp$mu
shapes.lspensp[, k] <- fit.lspensp$mu
shapes.huber[, k] <- fit.huber$mu
}
# Summarize Results
# Mean and standard error of MSE
mean(mse.smsp, na.rm = TRUE) * 1000; 1000 * sd(mse.smsp, na.rm = TRUE)/sqrt(nsim)
mean(mse.pensp, na.rm = TRUE) * 1000; 1000 * sd(mse.pensp, na.rm = TRUE)/sqrt(nsim)
mean(mse.lspensp, na.rm = TRUE) * 1000; 1000 * sd(mse.lspensp, na.rm = TRUE)/sqrt(nsim)
mean(mse.huber, na.rm = TRUE) * 1000; 1000 * sd(mse.huber, na.rm = TRUE)/sqrt(nsim)
# Visualize Estimated Curves Across Simulations
par(mar = c(4,3.5,2,2), mgp = c(1.5, 0.75, 0), mfrow = c(1,1))
matplot(t_grid, shapes.pensp, lwd = 3, col = "gray", type = "l", lty = 1, cex.lab = 1.5, cex.axis = 1.5,
ylab = "", xlab = "t", ylim = c(-1,1)) # ylim = c(-1,1) or ylim = c(-0.5,1.3)
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
par(mar = c(4,3.5,2,2), mgp = c(1.5, 0.75, 0), mfrow = c(1,1))
matplot(t_grid, shapes.lspensp, lwd = 3, col = "gray", type = "l", lty = 1,cex.lab = 1.5, cex.axis = 1.5,
ylab = "", xlab = "t", ylim = c(-1, 1))
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
par(mfrow = c(1, 1))
matplot(t_grid, shapes.huber, lwd = 3, col = "gray", type = "l", lty = 1)
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
source("quan_smsp.R")    # Quantile Smoothing Spline Estimator
source("quan_pensp.R")   # Quantile Penalized Spline Estimator
source("ls_pensp.R")    # Least-Squares Penalized Spline Estimator
source("huber_pensp.R") # Huber Penalized Spline estimator
```r
```r
```r
n
set.seed(2)
n    <- 100 # 50 for smaller samples
p    <- 50 # 30  for more sparsely observed data
t_grid <- seq(0, 1, length.out = p)
## Population mean
mu_true <- function(t) sin(2 * pi * t) # Easy
# mu_true <- function(t) exp(-(t-0.25)^2/0.01)+exp(-(t-0.50)^2/0.01) + exp(-(t-0.75)^2/0.01) # Much harder
mu_grid <- mu_true(t_grid)
X <- matrix(NA, nrow = n, ncol = p)
Y <- matrix(NA, nrow = n, ncol = p) #
for(i in 1:n){
X[i,] <- mu_grid
for(j in 1:50){
X[i,] <- X[i, ] +  sqrt(2)*rt(1, df = 5)*sapply(t_grid, FUN = function(x) sin((j-1/2)*pi*x)/((j-1/2)*pi) )
}
m_i <- sample(floor(0.5 * p):floor(0.8 * p), 1)
idx <- sort(sample(seq_len(p), m_i))
zeta <- 0.5*rt(m_i, df = 10)
Y[i, idx] <- X[i, idx] + zeta
}
fit.lspensp <- ls_pensp(Y) # Least squares penalized spline estimator
fit.pensp <- quan_pensp(Y) # LAD penalized spline estimator
par(mar = c(4,3.5,2,2), mgp = c(3, 1.5, 0))
plot(t_grid, mu_grid, lwd = 3, lty = 1, type = "l", cex.axis = 2.5, cex.lab = 2.5, ylab = "", xlab = "t",
ylim = c(-1.2, 1.2)) ; grid()
lines(t_grid,fit.pensp$mu, lwd = 3, type= "l", col = "blue")
lines(t_grid, fit.lspensp$mu, lwd = 3, type = "l", col = "red")
setwd("C:/Users/ik77w/OneDrive - University of Glasgow/Documents/GitHub/Robust-Functional-Location-Estimation")
source("quan_smsp.R")    # Quantile Smoothing Spline Estimator
source("quan_pensp.R")   # Quantile Penalized Spline Estimator
source("ls_pensp.R")    # Least-Squares Penalized Spline Estimator
source("huber_pensp.R") # Huber Penalized Spline estimator
setwd("C:/Users/ik77w/OneDrive - University of Glasgow/Documents/GitHub/Robust-Functional-Location-Estimation")
source("quan_smsp.R")    # Quantile Smoothing Spline Estimator
