X$trt <- X.all$trt[ -ii ]
empty.test <- (sapply(X.test$x, length) == 0)
empty.tr <- (sapply(X$x, length) == 0)
X$pp <- X$pp[!empty.tr]
X$x <- X$x[!empty.tr]
X.test$x <- X.test$x[ !empty.test ]
X.test$pp <- X.test$pp[ !empty.test ]
ra.tr <- range(unlist(X$pp))
ra.te <- range(unlist(X.test$pp))
ok.sample <- ( (ra.tr[1] < ra.te[1]) && (ra.te[2] < ra.tr[2]) )
}
if(!ok.sample) stop('Did not find good split')
ncpus <- 4
seed <- 123
rho.param <- 1e-3
max.kappa <- 1e3
ncov <- 50
k.cv <- 10
k <- 5
s <- k
hs.cov <- seq(1, 7, length=10)
hs.mu <- .3
ours.r.tr <- efpca(X=X, ncpus=ncpus, hs.mu=hs.mu, hs.cov=hs.cov, rho.param=rho.param, alpha=0.2,
k = k, s = k, trace=FALSE, seed=seed, k.cv=k.cv, ncov=ncov, max.kappa=max.kappa)
ours.ls.tr <- lsfpca(X=X, ncpus=ncpus, hs.mu=hs.mu, hs.cov=hs.cov, rho.param=rho.param,
k = k, s = k, trace=FALSE, seed=seed, k.cv=k.cv, ncov=ncov, max.kappa=max.kappa)
myop <- list(error=FALSE, methodXi='CE', dataType='Sparse',
userBwCov = 1.5, userBwMu= .3,
kernel='epan', verbose=FALSE, nRegGrid=50)
pace <- FPCA(Ly=X$x, Lt=X$pp, optns=myop)
plot(ours.ls$ma$mt[,1], ours.ls$ma$mt[,2], pch=19, col='gray70', cex=.8,
xlab='s', ylab='t', cex.lab=1.2, cex.axis=1.1)
points(ours.ls$ma$mt[,1], ours.ls$ma$mt[,1], pch=19, col='gray70', cex=.8)
ss <- tt <- ours.r$ss
G.r <- ours.r$cov.fun
filled.contour(tt, ss, G.r, main='ROB')
ss <- tt <- ours.ls$ss
G.ls <- ours.ls$cov.fun
filled.contour(tt, ss, G.ls, main='LS')
ss <- tt <- pace$workGrid
G.pace <- pace$smoothedCov
filled.contour(tt, ss, G.pace, main='PACE')
# pr2.pace <- predict(pace.tr, newLy = X.test$x, newLt=X.test$pp, K = ncol(pace.tr$xiEst), xiMethod='CE')
# pp.pace <- pace.tr$phi %*% t(pr2.pace)
pr2.pace <- predict(pace.tr, newLy = X.test$x, newLt=X.test$pp, K = ncol(pace.tr$xiEst), xiMethod='CE')
pp.pace <- pace.tr$phi %*% t(pr2.pace$scores)
pace.tr <- FPCA(Ly=X$x, Lt=X$pp, optns=myop)
# pr2.pace <- predict(pace.tr, newLy = X.test$x, newLt=X.test$pp, K = ncol(pace.tr$xiEst), xiMethod='CE')
# pp.pace <- pace.tr$phi %*% t(pr2.pace)
pr2.pace <- predict(pace.tr, newLy = X.test$x, newLt=X.test$pp, K = ncol(pace.tr$xiEst), xiMethod='CE')
pp.pace <- pace.tr$phi %*% t(pr2.pace$scores)
tts <- unlist(X$pp)
mus <- unlist(ours.ls.tr$muh)
mu.fn <- approxfun(x=tts, y=mus)
mu.fn.ls <- mu.fn(ours.ls.tr$tt)
kk <- 2
pred.test.ls <- pred.cv.whole(X=X, muh=mu.fn.ls, X.pred=X.test,
muh.pred=ours.ls$muh[ii],
cov.fun=ours.ls.tr$cov.fun, tt=ours.ls.tr$tt,
k=kk, s=kk, rho=ours.ls.tr$rho.param)
tts <- unlist(X$pp)
mus <- unlist(ours.r.tr$muh)
mu.fn <- approxfun(x=tts, y=mus)
mu.fn.r <- mu.fn(ours.r.tr$tt)
pred.test.r <- pred.cv.whole(X=X, muh=mu.fn.r, X.pred=X.test,
muh.pred=ours.r$muh[ii],
cov.fun=ours.r.tr$cov.fun, tt=ours.r.tr$tt,
k=kk, s=kk, rho=ours.r.tr$rho.param)
xmi <- min( tmp <- unlist(X$x) )
xma <- max( tmp )
ymi <- min( tmp <- unlist(X$pp) )
yma <- max( tmp )
ii2 <- 1:length(X$x)
show.these <- c(4, 44, 46, 34)
for(j in show.these) {
plot(seq(ymi, yma, length=5), seq(xmi, xma,length=5), type='n', xlab='t', ylab='X(t)')
lines(X.test$pp[[j]], X.test$x[[j]], col='gray50', lwd=5, type='b', pch=19, cex=2)
lines(pace.tr$workGrid, pp.pace[,j] + pace.tr$mu, lwd=3, lty=3)
lines(ours.ls.tr$tt, pred.test.ls[[j]], lwd=3, lty=2)
lines(ours.r.tr$tt, pred.test.r[[j]], lwd=3, lty=1)
legend('topright', legend=c('Robust (ROB)', 'Non-robust (LS)', 'PACE'), lwd=2, lty=1:3)
}
version
sessionInfo()
2+3+2+2+3+2+2+4
library(sparseFPCA)
efpca
# ----------------------------------------------------------------------
# Simulation Study: Section 4 of Kalogridis (2025+)
# ----------------------------------------------------------------------
# This script simulates discretely sampled functional data, adds
# noise, and compares three estimators:
#   1. Smoothing-spline quantile estimator (quan_smsp)
#   2. Least-squares penalized spline estimator (ls_pensp)
#   3. Quantile penalized spline estimator (quan_pensp)
#   4. Huber penalized spline estimator (huber_pensp)
# ----------------------------------------------------------------------
setwd("C:/Users/ik77w/OneDrive - University of Glasgow/Documents/GitHub/Robust-Functional-Location-Estimation")
# ----------------------------------------------------------------------
# quan_smsp: Quantile Smoothing Spline Estimator
# ----------------------------------------------------------------------
# This function estimates the alpha-th quantile function of discretely
# sampled functional data using B-splines and a roughness penalty.
# Computation is performed via fast C++ routines (IRLS + GCV).
# ----------------------------------------------------------------------
require(fda)           # For B-spline basis and penalty functions
require(Rcpp)          # Interface to C++
require(RcppArmadillo) # Efficient matrix operations in C++
Rcpp::sourceCpp("combined.cpp")  # Load the C++ functions
quan_smsp <- function(Y, alpha = 0.5, r = 2,
lambda_grid = exp(seq(log(1e-8), log(1e-1), length.out = 50)),
max_it = 200, tol = 1e-6, tun = 1e-4) {
# --- Preprocessing ---
# Convert input to matrix if needed and remove empty rows
Y <- as.matrix(Y)
Y <- Y[rowSums(!is.na(Y)) > 0, , drop = FALSE]
n <- nrow(Y)  # number of subjects
p <- ncol(Y)  # number of measurement points per subject
# Create a uniform grid for the measurements (0 to 1)
t_grid <- seq(0, 1, length.out = p)
# Map each observation to its time point
T_mat <- matrix(rep(t_grid, each = n), nrow = n)
obs_idx <- which(!is.na(Y))  # indices of observed entries
t_obs <- T_mat[obs_idx]      # time points of observed entries
y_obs <- Y[obs_idx]          # observed values
# --- Weights ---
# Compute number of measurements per subject
m_i <- rowSums(!is.na(Y))
# Map linear indices back to row IDs
row_id <- ((obs_idx - 1) %% n) + 1
# Assign weight 1/(n * m_i) to each observation
# Ensures subjects with fewer measurements are not overweighted
weights_per_obs <- 1 / (n * m_i[row_id])
# --- Basis Construction ---
# Use B-spline basis with knots at observed points and order 2*r
knots <- sort(unique(t_obs))
b_basis <- create.bspline.basis(rangeval = c(0, 1), breaks = knots, norder = 2 * r)
# Evaluate B-spline basis at observed times
B <- eval.basis(t_obs, b_basis)
# --- Penalty Matrix ---
# Roughness penalty on r-th derivative
Pen <- bsplinepen(b_basis, Lfdobj = r)
# --- IRLS + GCV ---
# Main C++ routine: iteratively reweighted least squares with GCV
fit <- irls_gcv_cpp_pensp(B, Pen, y_obs, weights_per_obs, alpha,
lambda_grid, max_it, tol, tun)
# Compute estimated quantile function on full grid
mu_est <- eval.basis(t_grid, b_basis) %*% fit$beta_hat
# --- Return Results ---
return(list(
mu = mu_est,       # estimated alpha-th quantile function
lambda = fit$lambda,  # selected smoothing parameter
weights = fit$weights # final IRLS weights
))
}
# ----------------------------------------------------------------------
# quan_pensp: Quantile Penalized Spline Estimator
# ----------------------------------------------------------------------
# This function estimates the conditional quantile function of
# discretely sampled functional data using B-splines and an O-spline roughness
# penalty. Computation is performed via a fast C++ routine.
# For details, please see the documentation below.
# ----------------------------------------------------------------------
require(fda)           # For B-spline basis and penalty functions
require(Rcpp)          # Interface to C++
require(RcppArmadillo) # Efficient matrix operations in C++
Rcpp::sourceCpp("combined.cpp")  # Load the C++ functions
quan_pensp <- function(Y, alpha = 0.5, r = 2, m = 4, K = NULL,
lambda_grid = exp(seq(log(1e-8), log(1e-1), length.out = 50)),
max_it = 200, tol = 1e-6, tun = 1e-4) {
# - Preprocessing -
# Convert input to matrix and remove rows with all NA
Y <- as.matrix(Y)
Y <- Y[rowSums(!is.na(Y)) > 0, , drop = FALSE]
n <- nrow(Y)  # number of subjects
p <- ncol(Y)  # maximum number of measurement points
# Uniform grid of measurement points from 0 to 1
t_grid <- seq(0, 1, length.out = p)
# Map observations to their time points
T_mat <- matrix(rep(t_grid, each = n), nrow = n)
obs_idx <- which(!is.na(Y))      # indices of observed entries
t_obs <- T_mat[obs_idx]          # time points of observed entries
y_obs <- Y[obs_idx]              # observed values
# --- Weights ---
# Number of measurements per subject
m_i <- rowSums(!is.na(Y))
K <- ifelse(is.null(K), min(35, max(m_i)), K)
# Map linear indices back to row IDs
row_id <- ((obs_idx - 1) %% n) + 1
# Assign weight 1/(n * m_i) to each observation
weights_per_obs <- 1 / (n * m_i[row_id])
# --- Basis Construction ---
# Create B-spline basis with K basis functions and order m
b_basis <- create.bspline.basis(rangeval = c(0, 1), nbasis = K, norder = m)
# Evaluate B-spline basis at observed points
B <- eval.basis(t_obs, b_basis)
# Penalty Matrix from the fda package
# Roughness penalty on r-th derivative
Pen <- bsplinepen(b_basis, Lfdobj = r)
# IRLS + GCV
# Fit the penalized quantile regression using the C++ routine
fit <- irls_gcv_cpp_pensp(B, Pen, y_obs, weights_per_obs, alpha,
lambda_grid, max_it, tol, tun)
# Estimated quantile function on full grid
mu_est <- eval.basis(t_grid, b_basis) %*% fit$beta_hat
return(list(
mu = mu_est,       # estimated quantile function
lambda = fit$lambda,  # selected smoothing parameter
weights = fit$weights, # observation weights used in IRLS
iter = fit$iterations
))
}
# ----------------------------------------------------------------------
# Simulation Study: Section 4 of Kalogridis (2025+)
# ----------------------------------------------------------------------
# This script simulates discretely sampled functional data, adds
# noise, and compares three estimators:
#   1. Smoothing-spline quantile estimator (quan_smsp)
#   2. Least-squares penalized spline estimator (ls_pensp)
#   3. Quantile penalized spline estimator (quan_pensp)
#   4. Huber penalized spline estimator (huber_pensp)
# ----------------------------------------------------------------------
setwd("C:/Users/ik77w/OneDrive - University of Glasgow/Documents/GitHub/Robust-Functional-Location-Estimation")
library(sparseFPCA)
# Number of simulations (500 in the paper)
nsim <- 500
# Number of subjects and number of measurement points
n <- 100 # 50
p <- 50 # 30
# Initialize storage for mean squared errors
mse.smsp <- mse.pensp <- mse.lspensp <- mse.huber <- rep(0, nsim)
mse.sfpca <- rep(0, nsim)
# Initialize storage for estimated curves
shapes.smsp <- shapes.pensp <- shapes.lspensp <- shapes.huber <- matrix(0, ncol = nsim, nrow = p)
shapes.sfpca <- matrix(0, ncol = nsim, nrow = p)
# Grid of measurement points
t_grid <- seq(0, 1, length.out = p)
# Define true population mean function
mu_true <- function(t) sin(2 * pi * t) # Easy example
# Harder example (mu_2 in the paper)
# mu_true <- function(t) exp(-(t-0.25)^2/0.01)+exp(-(t-0.50)^2/0.01)+exp(-(t-0.75)^2/0.01)
mu_grid <- mu_true(t_grid)
# Main Simulation Loop
for(k in 1:nsim){
print(paste("Simulation", k, "of", nsim))
# Generate functional data for n subjects
X <- matrix(NA, nrow = n, ncol = p) # discretely sampled data
Y <- matrix(NA, nrow = n, ncol = p) # discretely sampled data with measurement error
# DataSFPCA <- vector("list", 2)
# names(DataSFPCA) <- c("x", "pp")
for(i in 1:n){
X[i,] <- mu_grid
for(j in 1:50){
X[i,] <- X[i,] + sqrt(2) * rt(1, df = 5) *
sapply(t_grid, FUN = function(x) sin((j-0.5)*pi*x)/((j-0.5)*pi))
}
# Randomly sample a subset of measurement points for subject i
m_i <- sample(floor(0.5 * p):floor(0.8 * p), 1)
idx <- sort(sample(seq_len(p), m_i))
# Add heavy-tailed noise
zeta <- 0.5 * rt(m_i, df = 2)
# zeta <- 0
Y[i, idx] <- X[i, idx] + zeta
# DataSFPCA$x[[i]] <- Y[i, idx]
# DataSFPCA$pp[[i]] <- idx/p
}
# fit.sfpca <- tryCatch(
#   efpca(
#     X       = DataSFPCA,
#     hs.mu   = exp(seq(log(1e-04), log(1), length.out = 20)),
#     opt.h.cov = 4
#   ),
#   error = function(e) list(muh = NA)
# )
# fit.sfpca <- tryCatch(efpca(X=DataSFPCA, hs.mu = exp(seq(log(1e-04), log(1), length.out = 20)), opt.h.cov = 4),
# error = function(e) list(a = NA, b = NA))
# u1 <- try(unlist(fit.sfpca$muh))      # all elements of list2
# u2 <- try(unlist(DataSFPCA$pp))
# u1 <- tryCatch(
#   unlist(fit.sfpca$muh),
#   error = function(e) rep(NA, length(DataSFPCA$pp))   # correct length placeholder
# )
#
# u2 <- tryCatch(
#   unlist(DataSFPCA$pp),
#   error = function(e) rep(NA, length(u1))
# )
# sorted_unique <- tryCatch(
#   sort(unique(u2)),
#   error = function(e) NA
# )
## ----- 4. Compute mu.sfpca safely -----
# mu.sfpca <- tryCatch(
#   u1[match(sorted_unique, u2)],
#   error = function(e) rep(NA, length(sorted_unique))
# )
# sorted_unique <- try(sort(unique(u2)))
# mu.sfpca <- try(u1[match(sorted_unique, u2)])
# plot(t_grid, mu.sfpca)
# # par(mar = c(4,3.5,2,2), mgp = c(1.5, 0.75, 0), mfrow = c(1,1))
# matplot(t_grid,t(X), pch = 20, col = "gray", cex = 1.1, xlab = "t",
# ylab = "", ylim = c(-4.5, 4.5), cex.lab = 1.5, cex.axis = 1.5) ; grid()
# lines(t_grid, mu_grid, lwd = 3, col = "black")
# Fit estimators
fit.smsp <- quan_smsp(Y, alpha = 0.5)
# fit.lspensp <- ls_pensp(Y)            # Least-squares penalized spline
fit.pensp  <- quan_pensp(Y, alpha = 0.5)  # Quantile penalized spline
# fit.huber <- huber_pensp(Y)
# -------------------------------
# Plot a single simulation
# -------------------------------
# par(mar = c(4,3.5,2,2), mgp = c(3, 1.5, 0), mfrow = c(1,1))
# plot(t_grid, mu_grid, lwd = 3, lty = 1, type = "l", cex.axis = 2.5, cex.lab = 2.5,
# ylab = "", xlab = "t", ylim = c(-1.2, 1.2)); grid()
# lines(t_grid, fit.huber$mu, lwd = 3, col = "blue")
# lines(t_grid, fit.pensp$mu, lwd = 3, col = "blue")
# lines(t_grid, fit.lspensp$mu, lwd = 3, col = "red")
# -------------------------------
# Compute Mean Squared Errors
# -------------------------------
mse.smsp[k] <- mean((fit.smsp$mu - mu_grid)^2)
mse.pensp[k] <- mean((fit.pensp$mu - mu_grid)^2)
# mse.lspensp[k] <- mean((fit.lspensp$mu - mu_grid)^2)
# mse.huber[k] <- mean((fit.huber$mu - mu_grid)^2)
# mse.sfpca[k] <- try(mean((mu.sfpca- mu_grid)^2))
# mse.sfpca[k] <- tryCatch(
#   mean((mu.sfpca - mu_grid)^2),
#   error = function(e) NA
# )
# Store estimated curves
shapes.smsp[, k] <- fit.smsp$mu
shapes.pensp[, k] <- fit.pensp$mu
# shapes.lspensp[, k] <- fit.lspensp$mu
# shapes.huber[, k] <- fit.huber$mu
# shapes.sfpca[, k] <- try(mu.sfpca)
# shapes.sfpca[, k] <- tryCatch(
#   mu.sfpca,
#   error = function(e) rep(NA, nrow(shapes.sfpca))
# )
}
# mean(mse.sfpca[mse.sfpca!=0])*1000; 1000 * sd(mse.sfpca[mse.sfpca!=0], na.rm = TRUE)/sqrt(nsim)
# Summarize Results
# Mean and standard error of MSE
mean(mse.smsp, na.rm = TRUE) * 1000; 1000 * sd(mse.smsp, na.rm = TRUE)/sqrt(nsim)
mean(mse.pensp, na.rm = TRUE) * 1000; 1000 * sd(mse.pensp, na.rm = TRUE)/sqrt(nsim)
mean(mse.lspensp, na.rm = TRUE) * 1000; 1000 * sd(mse.lspensp, na.rm = TRUE)/sqrt(nsim)
mean(mse.huber, na.rm = TRUE) * 1000; 1000 * sd(mse.huber, na.rm = TRUE)/sqrt(nsim)
# Visualize Estimated Curves Across Simulations
par(mar = c(4,3.5,2,2), mgp = c(1.5, 0.75, 0), mfrow = c(1,1))
matplot(t_grid, shapes.pensp, lwd = 3, col = "gray", type = "l", lty = 1, cex.lab = 1.5, cex.axis = 1.5,
ylab = "", xlab = "t", ylim = c(-1,1)) # ylim = c(-1,1) or ylim = c(-0.5,1.3)
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
par(mar = c(4,3.5,2,2), mgp = c(1.5, 0.75, 0), mfrow = c(1,1))
matplot(t_grid, shapes.lspensp, lwd = 3, col = "gray", type = "l", lty = 1,cex.lab = 1.5, cex.axis = 1.5,
ylab = "", xlab = "t", ylim = c(-1, 1))
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
par(mfrow = c(1, 1))
matplot(t_grid, shapes.huber, lwd = 3, col = "gray", type = "l", lty = 1)
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
# ----------------------------------------------------------------------
# Simulation Study: Section 4 of Kalogridis (2025+)
# ----------------------------------------------------------------------
# This script simulates discretely sampled functional data, adds
# noise, and compares three estimators:
#   1. Smoothing-spline quantile estimator (quan_smsp)
#   2. Least-squares penalized spline estimator (ls_pensp)
#   3. Quantile penalized spline estimator (quan_pensp)
#   4. Huber penalized spline estimator (huber_pensp)
# ----------------------------------------------------------------------
setwd("C:/Users/ik77w/OneDrive - University of Glasgow/Documents/GitHub/Robust-Functional-Location-Estimation")
library(sparseFPCA)
# Number of simulations (500 in the paper)
nsim <- 500
# Number of subjects and number of measurement points
n <- 100 # 50
p <- 50 # 30
# Initialize storage for mean squared errors
mse.smsp <- mse.pensp <- mse.lspensp <- mse.huber <- rep(0, nsim)
mse.sfpca <- rep(0, nsim)
# Initialize storage for estimated curves
shapes.smsp <- shapes.pensp <- shapes.lspensp <- shapes.huber <- matrix(0, ncol = nsim, nrow = p)
shapes.sfpca <- matrix(0, ncol = nsim, nrow = p)
# Grid of measurement points
t_grid <- seq(0, 1, length.out = p)
# Define true population mean function
# mu_true <- function(t) sin(2 * pi * t) # Easy example
# Harder example (mu_2 in the paper)
mu_true <- function(t) exp(-(t-0.25)^2/0.01)+exp(-(t-0.50)^2/0.01)+exp(-(t-0.75)^2/0.01)
mu_grid <- mu_true(t_grid)
# Main Simulation Loop
for(k in 1:nsim){
print(paste("Simulation", k, "of", nsim))
# Generate functional data for n subjects
X <- matrix(NA, nrow = n, ncol = p) # discretely sampled data
Y <- matrix(NA, nrow = n, ncol = p) # discretely sampled data with measurement error
# DataSFPCA <- vector("list", 2)
# names(DataSFPCA) <- c("x", "pp")
for(i in 1:n){
X[i,] <- mu_grid
for(j in 1:50){
X[i,] <- X[i,] + sqrt(2) * rt(1, df = 5) *
sapply(t_grid, FUN = function(x) sin((j-0.5)*pi*x)/((j-0.5)*pi))
}
# Randomly sample a subset of measurement points for subject i
m_i <- sample(floor(0.5 * p):floor(0.8 * p), 1)
idx <- sort(sample(seq_len(p), m_i))
# Add heavy-tailed noise
zeta <- 0.5 * rt(m_i, df = 2)
# zeta <- 0
Y[i, idx] <- X[i, idx] + zeta
# DataSFPCA$x[[i]] <- Y[i, idx]
# DataSFPCA$pp[[i]] <- idx/p
}
# fit.sfpca <- tryCatch(
#   efpca(
#     X       = DataSFPCA,
#     hs.mu   = exp(seq(log(1e-04), log(1), length.out = 20)),
#     opt.h.cov = 4
#   ),
#   error = function(e) list(muh = NA)
# )
# fit.sfpca <- tryCatch(efpca(X=DataSFPCA, hs.mu = exp(seq(log(1e-04), log(1), length.out = 20)), opt.h.cov = 4),
# error = function(e) list(a = NA, b = NA))
# u1 <- try(unlist(fit.sfpca$muh))      # all elements of list2
# u2 <- try(unlist(DataSFPCA$pp))
# u1 <- tryCatch(
#   unlist(fit.sfpca$muh),
#   error = function(e) rep(NA, length(DataSFPCA$pp))   # correct length placeholder
# )
#
# u2 <- tryCatch(
#   unlist(DataSFPCA$pp),
#   error = function(e) rep(NA, length(u1))
# )
# sorted_unique <- tryCatch(
#   sort(unique(u2)),
#   error = function(e) NA
# )
## ----- 4. Compute mu.sfpca safely -----
# mu.sfpca <- tryCatch(
#   u1[match(sorted_unique, u2)],
#   error = function(e) rep(NA, length(sorted_unique))
# )
# sorted_unique <- try(sort(unique(u2)))
# mu.sfpca <- try(u1[match(sorted_unique, u2)])
# plot(t_grid, mu.sfpca)
# # par(mar = c(4,3.5,2,2), mgp = c(1.5, 0.75, 0), mfrow = c(1,1))
# matplot(t_grid,t(X), pch = 20, col = "gray", cex = 1.1, xlab = "t",
# ylab = "", ylim = c(-4.5, 4.5), cex.lab = 1.5, cex.axis = 1.5) ; grid()
# lines(t_grid, mu_grid, lwd = 3, col = "black")
# Fit estimators
fit.smsp <- quan_smsp(Y, alpha = 0.5)
# fit.lspensp <- ls_pensp(Y)            # Least-squares penalized spline
fit.pensp  <- quan_pensp(Y, alpha = 0.5)  # Quantile penalized spline
# fit.huber <- huber_pensp(Y)
# -------------------------------
# Plot a single simulation
# -------------------------------
# par(mar = c(4,3.5,2,2), mgp = c(3, 1.5, 0), mfrow = c(1,1))
# plot(t_grid, mu_grid, lwd = 3, lty = 1, type = "l", cex.axis = 2.5, cex.lab = 2.5,
# ylab = "", xlab = "t", ylim = c(-1.2, 1.2)); grid()
# lines(t_grid, fit.huber$mu, lwd = 3, col = "blue")
# lines(t_grid, fit.pensp$mu, lwd = 3, col = "blue")
# lines(t_grid, fit.lspensp$mu, lwd = 3, col = "red")
# -------------------------------
# Compute Mean Squared Errors
# -------------------------------
mse.smsp[k] <- mean((fit.smsp$mu - mu_grid)^2)
mse.pensp[k] <- mean((fit.pensp$mu - mu_grid)^2)
# mse.lspensp[k] <- mean((fit.lspensp$mu - mu_grid)^2)
# mse.huber[k] <- mean((fit.huber$mu - mu_grid)^2)
# mse.sfpca[k] <- try(mean((mu.sfpca- mu_grid)^2))
# mse.sfpca[k] <- tryCatch(
#   mean((mu.sfpca - mu_grid)^2),
#   error = function(e) NA
# )
# Store estimated curves
shapes.smsp[, k] <- fit.smsp$mu
shapes.pensp[, k] <- fit.pensp$mu
# shapes.lspensp[, k] <- fit.lspensp$mu
# shapes.huber[, k] <- fit.huber$mu
# shapes.sfpca[, k] <- try(mu.sfpca)
# shapes.sfpca[, k] <- tryCatch(
#   mu.sfpca,
#   error = function(e) rep(NA, nrow(shapes.sfpca))
# )
}
# mean(mse.sfpca[mse.sfpca!=0])*1000; 1000 * sd(mse.sfpca[mse.sfpca!=0], na.rm = TRUE)/sqrt(nsim)
# Summarize Results
# Mean and standard error of MSE
mean(mse.smsp, na.rm = TRUE) * 1000; 1000 * sd(mse.smsp, na.rm = TRUE)/sqrt(nsim)
mean(mse.pensp, na.rm = TRUE) * 1000; 1000 * sd(mse.pensp, na.rm = TRUE)/sqrt(nsim)
mean(mse.lspensp, na.rm = TRUE) * 1000; 1000 * sd(mse.lspensp, na.rm = TRUE)/sqrt(nsim)
mean(mse.huber, na.rm = TRUE) * 1000; 1000 * sd(mse.huber, na.rm = TRUE)/sqrt(nsim)
# Visualize Estimated Curves Across Simulations
par(mar = c(4,3.5,2,2), mgp = c(1.5, 0.75, 0), mfrow = c(1,1))
matplot(t_grid, shapes.pensp, lwd = 3, col = "gray", type = "l", lty = 1, cex.lab = 1.5, cex.axis = 1.5,
ylab = "", xlab = "t", ylim = c(-1,1)) # ylim = c(-1,1) or ylim = c(-0.5,1.3)
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
par(mar = c(4,3.5,2,2), mgp = c(1.5, 0.75, 0), mfrow = c(1,1))
matplot(t_grid, shapes.lspensp, lwd = 3, col = "gray", type = "l", lty = 1,cex.lab = 1.5, cex.axis = 1.5,
ylab = "", xlab = "t", ylim = c(-1, 1))
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
par(mfrow = c(1, 1))
matplot(t_grid, shapes.huber, lwd = 3, col = "gray", type = "l", lty = 1)
lines(t_grid, mu_grid, lwd = 3, col = "black")
grid()
