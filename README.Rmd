---
title: "Penalized Spline M-Estimators for Discretely Sampled Functional
Data"
output: github_document
---

This repository contains fast ```C++``` implementations with an ```R``` interface of the penalized spline estimators of Kalogridis (2025+) as well as a remake of the older smoothing spline estimator of
Kalogridis and Van Aelst (2023, SJS). 

The computation is done with **Iteratively Reweighted Least-Squares** and the penalty parameter is selected with **robust Generalized Cross Validation**.

Here are detailed installation instructions:

1. First, download all the files in your ```R``` working directory. This is at:

```{r echo=TRUE, message=FALSE, results = "hide"}
getwd()
```
2. Load the ```R```-functions ```quan_smsp.R``` (Quantile Smoothing Spline Estimator), ```quan_pensp.R``` (Quantile Penalized Spline Estimator) and ```ls_pensp.R``` (Least-Squares Penalized Spline Estimator).

```{r message = FALSE}
source("quan_smsp.R")    # Quantile Smoothing Spline Estimator
source("quan_pensp.R")   # Quantile Penalized Spline Estimator
source("ls_pensp.R")    # Least-Squares Penalized Spline Estimator
source("huber_pensp.R") # Huber Penalized Spline estimator
```

3. Be sure to have installed and loaded the ```R```-packages ```fda```, ```Rcpp``` and ```RcppArmadillo```:

```{r, eval = FALSE}
# install.packages(c("fda", "Rcpp", "RcppArmadillo"))
library(fda);library(Rcpp);library(RcppArmadillo)
```

4. These ```R``` functions will source the ```combined.cpp``` file containing the ```C++``` implementations; no ```C++``` knowledge is required.

## Example: Simulated Data

All examples below use simulated discretely sampled functional data. No external datasets are required.

```{r message=FALSE, warning = FALSE, cache=TRUE}
set.seed(2)
n    <- 100 # 50 for smaller samples
p    <- 50 # 30  for more sparsely observed data
t_grid <- seq(0, 1, length.out = p)

## Population mean
mu_true <- function(t) sin(2 * pi * t) # Easy
# mu_true <- function(t) exp(-(t-0.25)^2/0.01)+exp(-(t-0.50)^2/0.01) + exp(-(t-0.75)^2/0.01) # Much harder

mu_grid <- mu_true(t_grid)
X <- matrix(NA, nrow = n, ncol = p) # discretely sample functional data
Y <- matrix(NA, nrow = n, ncol = p) # with measurement error 
for(i in 1:n){
  X[i,] <- mu_grid 
  for(j in 1:50){ 
    X[i,] <- X[i, ] +  sqrt(2)*rt(1, df = 5)*sapply(t_grid, FUN = function(x) sin((j-1/2)*pi*x)/((j-1/2)*pi) )
  }
  m_i <- sample(floor(0.5 * p):floor(0.8 * p), 1)
  idx <- sort(sample(seq_len(p), m_i))
  zeta <- 0.5*rt(m_i, df = 10)
  Y[i, idx] <- X[i, idx] + zeta}

fit.lspensp <- ls_pensp(Y) # Least squares penalized spline estimator
fit.pensp <- quan_pensp(Y) # LAD penalized spline estimator
```

```{r message=FALSE, warning=FALSE, fig.width=8, fig.height=5, dpi=120}
library(ggplot2)
library(viridis)

df <- data.frame(
  t = rep(t_grid, 3),
  value = c(mu_grid, fit.pensp$mu, fit.lspensp$mu),
  method = factor(rep(c("True","LAD Pen. Spline","LS Pen. Spline"), each=length(t_grid)),
  levels = c("True","LAD Pen. Spline","LS Pen. Spline")))

colors <- c("True" = "black", "LAD Pen. Spline" = "red", "LS Pen. Spline" = "blue")
line_types <- c("True" = "solid", "LAD Pen. Spline" = "longdash", "LS Pen. Spline" = "twodash")

ggplot(df, aes(x=t, y=value, color=method, linetype=method)) +
  geom_line(size=1.2) +
  theme_minimal(base_size = 16) +
  labs(x="t", y="", title="") + coord_cartesian(ylim=c(-1.2,1.2)) + 
  scale_color_manual(values = colors) +
  scale_linetype_manual(values = line_types) + 
  theme(legend.title = element_blank(),
        legend.position = "bottom",
        plot.title = element_text(hjust = 0.5)
  )
```

If the measurement errors follow a light-tailed distribution, the estimators perform comparably. 

But for heavier tailed measurement errors the situation changes dramatically:

```{r message=FALSE, warning = FALSE, cache=TRUE}
set.seed(2)
X <- matrix(NA, nrow = n, ncol = p) 
Y <- matrix(NA, nrow = n, ncol = p) 
for(i in 1:n){
  X[i,] <- mu_grid 
  for(j in 1:50){ 
    X[i,] <- X[i, ] +  sqrt(2)*rt(1, df = 5)*sapply(t_grid, FUN = function(x) sin((j-1/2)*pi*x)/((j-1/2)*pi) )
  }
  m_i <- sample(floor(0.5 * p):floor(0.8 * p), 1)
  idx <- sort(sample(seq_len(p), m_i))
  zeta <- 0.5*rt(m_i, df = 1)
  Y[i, idx] <- X[i, idx] + zeta}

fit.lspensp <- ls_pensp(Y)
fit.pensp <- quan_pensp(Y) 
```

```{r message=FALSE, warning=FALSE, fig.width=8, fig.height=5, dpi=120}

df <- data.frame(
  t = rep(t_grid, 3),
  value = c(mu_grid, fit.pensp$mu, fit.lspensp$mu),
  method = factor(rep(c("True","LAD Pen. Spline","LS Pen. Spline"), each=length(t_grid)),
  levels = c("True","LAD Pen. Spline","LS Pen. Spline")))

ggplot(df, aes(x=t, y=value, color=method, linetype=method)) +
  geom_line(size=1.2) +
  theme_minimal(base_size = 16) +
  labs(x="t", y="", title="") + coord_cartesian(ylim=c(-1.2,1.2)) + 
  scale_color_manual(values = colors) +
  scale_linetype_manual(values = line_types) + 
  theme( legend.title = element_blank(),
         legend.position = "bottom",
         plot.title = element_text(hjust = 0.5))
```

## Notes

- Please see the ```R```-functions for complete documentation of the settings/options.
- The simulation script reproduces the results from Section 5 in Kalogridis (2025+).

Get in contact with me at ioannis.kalogridis@glasgow.ac.uk for any issues, questions or suggestions.

## References
1. Kalogridis, I. (2025+) Penalized Spline M-Estimators for Discretely Sampled Functional Data: Existence and Asymptotics, under review.
2. Kalogridis, I. and Van Aelst, S. (2023) Robust Optimal Estimation of Location from Discretely Sampled Functional Data, Scand. J. Statist. (50), 411--451.
